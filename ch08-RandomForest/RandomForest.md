### 一、 集成学习

在有监督学习算法中，其目标是输出一个稳定的且在各个方面表现良好的模型，但实际情况往往并不理想，有时我们只能得到在某些方面表现比较好的弱监督模型。于是人们设想能否博采众长，“三个臭皮匠顶个诸葛亮”，将多个弱监督模型组合成一个强监督模型？于是就产生了集成学习。

集成学习本身不是一个单独的机器学习算法，而是结合数个“好而不同”的机器学习模型，形成一个新的模型，以此来降方差，减偏差，提升预测准确性。集成学习可以用于分类问题集成，回归问题集成，特征选取集成，异常点检测集成等等，可以说所有的机器学习领域都可以看到集成学习的身影。在许多著名的机器学习竞赛中，集成学习方法也总是被优先考虑。

随着集成学习研究领域的深入发展, 不断有新的集成学习算法提出, 但是这些算法大都是由经典算法如Bagging、Boosting、Stacking等改进得到的, 改进的方向一般在于以下3个方面：提供给个体学习器的训练数据不同；产生个体学习器的过程不同; 学习结果的组合方式不同。所以下面来学习这三种经典算法。



### 二、 集成学习之bagging
![avatar](https://github.com/Miraclelucy/ml_in_action/blob/main/img/ch08/1.png?raw=true)

Bagging算法是最早的集成学习算法之一, 它结构简单, 但表现优越。该算法通过随机改变训练集的分布产生新的训练子集, 然后分别用不同的训练子集来训练个体学习器，最后将其集成为整体。Bagging算法的弱学习器之间没有依赖关系，可以并行工作。从上图可以看出，通过T次的随机采样，我们就可以得到T个采样集，对于这T个采样集，我们可以分别独立的训练出T个弱学习器，再对这T个弱学习器通过集成策略来得到最终的强学习器。

采样算法一般使用自助采样法（Bootstap sampling）来产生训练子集, 即先随机抽取出一个样本放入采样集中，再把该样本放回初始数据集，使得下一次采样时仍然有可能抽取到这个样本。这样一些实例会被多次采样, 而其他实例会被忽略, 因此, 对于特定的子空间, 个体学习器会具有很高的分类精度, 而对于那些被忽略的部分, 个体学习器难以正确分类。但是, 最终的预测结果是由多个个体学习器投票产生的, 所以当个体学习器某些方面效果越好且它们之间的差异越大时, 该集成算法的最终效果就会越好。由于不稳定的学习算法对于训练集比较敏感, 训练集只要产生一些微小的变化, 就会导致其预测结果发生很大的改变, 所以Bagging算法对于不稳定学习算法非常有效。

从偏差-方差分解的角度看，Bagging主要关注降低方差。Bagging通过重采样方法从原始训练集中有放回的采样得到多个训练子集, 由于各个训练子集相互独立，降低了基分类器的方差, 改善了泛化误差, 并且重采样方法可以有效降低原始训练集中随机波动导致的误差, 使得不稳定的学习器具有更好的学习效果。而且各个基学习器可以并行生成, 提高运行效率。


### 三、集成学习之boosting  
![avatar](https://github.com/Miraclelucy/ml_in_action/blob/main/img/ch08/2.png?raw=true)

Boosting算法通过迭代的方法将弱学习器转换为强学习器, 它通过增加迭代次数, 产生一个表现优秀的强学习器。对于监督学习, 该算法在第一个分类器之后产生的每一个分类器都是针对前一次未被正确分类的样本进行学习, 因此该算法可以有效地降低模型的偏差, 但随着训练的进行, 整体模型在训练集上的准确率不断提高, 导致方差变大, 通过对特征的随机采样可以降低分类模型间的相关性, 从而降低模型整体的方差。

Boosting算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。然后基于调整权重后的训练集来训练弱学习器2，如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。

从偏差-方差分解的角度看，Boosting主要关注降低偏差。Boosting在每轮训练中使用的训练集不变, 但训练集中每个样例会根据上一轮的学习结果进行调整, 使新学习器针对已有学习器判断错误的样本进行学习。这种方法能够显著提高弱学习器的学习效果, 但很容易受到噪声的影响产生过拟合, 并且每个基学习器只能顺序生成, 训练效率相对较低。

Boosting系列算法里最著名算法主要有AdaBoost算法和提升树(boosting tree)系列算法。提升树系列算法里面应用最广泛的是梯度提升树(Gradient Boosting Tree)。

### 四、集成学习之Stacking
![avatar](https://github.com/Miraclelucy/ml_in_action/blob/main/img/ch08/3.png?raw=true)

Stacking也称Stacked Generalization，指训练一个用于组合所有个体学习器的模型，即首先训练多个不同的个体学习器，然后再以这些个体学习器的输出作为输入来训练一个模型，从而得到一个最终的输出。

如图所示, 首先在整个训练数据集上通过重采样方法得到多个训练子集, 然后使用这些新产生的训练集训练得到一系列分类模型, 称之为Tier 1, 然后将Tier1的输出组合后用于训练Tier2的元分类器。Stacking使用初级学习器产生新的训练集来训练次级学习器，但如果直接使用初级学习器的训练集来产生次级训练集会有很大的过拟合风险, 因此通常使用交叉验证方法来产生次级训练集。即首先将训练集分成N等份, 然后Tier1中的每个个体学习器根据前N-1份训练集进行训练, 最后在第N份训练集上测试。



### 五、集成学习之集成策略

在前面的讨论我们主要关注于学习器，本节就对集成学习之集成策略做一个总结。我们假定我们有T个弱学习器，他们之间的集成策略通常会有平均法、投票法和学习法。

1、平均法

平均法是一种集成连续数值型输出常用的方法, 它主要分为简单平均法和加权平均法.  
![avatar](https://github.com/Miraclelucy/ml_in_action/blob/main/img/ch08/4.png?raw=true)

2、投票法

对于分类问题的预测，我们通常使用的是投票法。

(1) 绝对多数投票：当某标签的得票数超过个体学习器数量的一半以上时, 将该标签作为预测结果输出, 若不存在得票数超过一半的标签，则拒绝预测。

(2) 相对多数投票：取得票数最多的标签作为预测结果输出, 不需要考虑得票数是否超过个体学习器数量的一半, 若多个标签都获得最高的得票数, 则随机选择其中一个作为输出。

(3) 加权投票法：和加权平均法一样，每个弱学习器的分类票数要乘以一个权重，最终将各个类别的加权票数求和，最大的值对应的标签为最终类别。


3、学习法

对弱学习器的结果做平均或者投票，相对比较简单粗暴，但是可能学习误差较大，于是就有了学习法这种方法，学习法的代表方法是stacking，当使用stacking的结合策略时，我们不是对弱学习器的结果做简单的逻辑处理，而是再加上一层学习器，也就是说，我们将训练集弱学习器的学习结果作为下一级学习器的输入，重新训练一个学习器来得到最终结果。


### 六、随机森林

前面我们讲了集成学习，前几期又讲了决策树（Decision Tree），那么会很容易理解随机森林。随机森林是bagging的一个特化进阶版，所谓的特化是因为随机森林的弱学习器都是决策树。所谓的进阶是指随机森林在bagging的样本随机采样基础上，又加上了特征的随机选择，但其基本思想没有脱离bagging的范畴。随机森林有两个关键词，一个是“随机”，一个是“森林”。“森林”我们很好理解，一棵树叫做树，成百上千棵树就叫做森林，这样的比喻还是很贴切的，这也是随机森林的主要思想--集成思想的体现。“随机”的含义我们可以从两方面来理解。

一：随机选取数据集

比如我们想要构造 10 棵树。这些树在构造时彼此完全独立，算法对每棵树的数据集进行不同的随机选择，以确保树和树之间是有区别的。

二：随机选取特征

在每个结点处，算法随机选择特征的一个子集，并对其中一个特征寻找最佳测试，而不是对每个结点都寻找最佳测试。每个结点中特征子集的选择是相互独立的，这样树的每个结点可以使用特征的不同子集来做出决策。


随机森林是一种很灵活实用的方法，它有如下几个特点：

1.具有极好的准确率

2.能够有效地运行在大数据集上

3.能够处理具有高维特征的输入样本，而且不需要降维

4.能够评估各个特征在分类问题上的重要性

5.在生成过程中，能够获取到内部生成误差的一种无偏估计

6.对于缺省值问题也能够获得很好得结果

实际上，随机森林的特点不只有这六点，它就相当于机器学习领域的多面手，你几乎可以把任何东西扔进去，它基本上都是可供使用的。

