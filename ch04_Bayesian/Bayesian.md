机器学习中的朴素贝叶斯算法就是基于贝叶斯定理来做分类。贝叶斯定理是一个已经有300年历史的古老定理。其推导过程这里就不去重复了，大家可以自己学习。

 
我们先来熟悉几个概念


联合概率：包含多个条件，所有条件同时成立的概率，记为P(A,B)或者P(A∩B)，也就是A和B同时发生的概率

条件概率（又叫似然概率）：事件B发生的条件下事件A发生的概率，记作P(A|B)；多个条件事件的时候，各个事件的条件需要相互独立，特性P(A1,A2|B)=P(A1|B)P(A2|B)，此概率公式成立的条件是A1,A2是互相独立的。

先验概率：单独事件发生的概率，如 P(A)、P(B)。是指根据以往经验和分析得到的概率，常见的是基于历史数据的统计，可以由背景常识得出，也可以是人的主观观点给出。

后验概率：若 P(X|Y) 为正向，则 P(Y|X) 为反向，基于先验概率求得的反向条件概率，形式上与条件概率相同

 

贝叶斯公式：



这里：

P(A|B)是后验概率，一般是我们求解的目标。

P(B|A)是条件概率，又叫似然概率，一般是通过历史数据统计得到。一般不把它叫做先验概率，但从定义上也符合先验定义。

P(A)是先验概率，一般都是人主观给出的。贝叶斯中的先验概率一般特指它。

P(B)也是先验概率，只是在贝叶斯的很多应用中不重要（因为只要最大后验不求绝对值），需要时往往用全概率公式计算得到。

所以总的来说P(B|A)、P(A)、P(B)都可以视作先验概率，也就是可以通过历史数据统计得到，这样就为我们求解P(A|B)提供了基础。

此外机器学习中贝叶斯算法之所以在前面加个“朴素”就在于假设属性之间的相互独立性，属性相关性较大的话分类效果会打折扣。

 

这个理解起来实在晦涩，还是先来看两个例子吧

 

一口袋里有3只红球、2只白球，采用不放回方式摸取，求：
⑴ 第一次摸到红球（记作A）的概率；
⑵ 第二次摸到红球（记作B）的概率；
⑶ 已知第二次摸到了红球，求第一次摸到的是红球的概率。
解：
⑴ P(A)=3/5，这就是先验概率；
⑵ P(B)=P(A)P(B|A)+P(A逆)P(B|A逆)=3/5；
⑶ P(A|B)=P(A)P(B|A)/P(B)=1/2，这就是后验概率。

 

似乎还是有点难懂，再来继续看一个复杂点的朴素贝叶斯分类应用。已知有如下的天气记录与是否打网球的关系记录表。
